{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ozDWJlANyyo2"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "from cupy.cuda import nccl\n",
        "\n",
        "WORLD = 1\n",
        "DEVS = list(range(WORLD))\n",
        "\n",
        "unique_id = nccl.get_unique_id()\n",
        "comms = []\n",
        "streams = []\n",
        "for r in DEVS:\n",
        "    cp.cuda.Device(r).use()\n",
        "    stream = cp.cuda.Stream()\n",
        "    comm = nccl.NcclCommunicator(WORLD, unique_id, r)\n",
        "    comms.append(comm)\n",
        "    streams.append(stream)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_to_all():\n",
        "    elems_total = 1 << 16\n",
        "    elems_chunk = elems_total // WORLD\n",
        "\n",
        "    sendbuf, recvbuf = [], []\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        send = cp.arange(elems_total, dtype=cp.float32) + r * 1000\n",
        "        recv = cp.zeros_like(send)\n",
        "        sendbuf.append(send)\n",
        "        recvbuf.append(recv)\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        comm = comms[r]\n",
        "        stream = streams[r]\n",
        "        nccl.groupStart()\n",
        "        for peer in DEVS:\n",
        "            s_off = peer * elems_chunk\n",
        "            r_off = peer * elems_chunk\n",
        "            s_view = sendbuf[r][s_off : s_off + elems_chunk]\n",
        "            r_view = recvbuf[r][r_off : r_off + elems_chunk]\n",
        "            # send/recv(ptr, count, dtype, peer, stream)\n",
        "            comm.send(s_view.data.ptr, elems_chunk, nccl.NCCL_FLOAT32, peer, stream.ptr)\n",
        "            comm.recv(r_view.data.ptr, elems_chunk, nccl.NCCL_FLOAT32, peer, stream.ptr)\n",
        "        nccl.groupEnd()\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        streams[r].synchronize()\n",
        "    print(\"All-to-all OK\")"
      ],
      "metadata": {
        "id": "2XKEgPLVyznC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemm_reducescatter():\n",
        "    M, K, N = 1024, 512, 1024\n",
        "    rows_per_rank = M // WORLD\n",
        "\n",
        "    A, B, Cpartial, Cstrip = [], [], [], []\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        A_r = cp.random.rand(M, K, dtype=cp.float32)\n",
        "        B_r = cp.random.rand(K, N, dtype=cp.float32)\n",
        "        C_r = A_r @ B_r                                   # (M, N)\n",
        "        C_out = cp.zeros((rows_per_rank, N), dtype=cp.float32)\n",
        "        A.append(A_r); B.append(B_r)\n",
        "        Cpartial.append(C_r); Cstrip.append(C_out)\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        comm = comms[r]\n",
        "        stream = streams[r]\n",
        "        comm.reduceScatter(\n",
        "            Cpartial[r].ravel().data.ptr,                 # send buffer (size = recvcount*WORLD)\n",
        "            Cstrip[r].ravel().data.ptr,                   # recv buffer\n",
        "            rows_per_rank * N,                            # recvcount (elements)\n",
        "            nccl.NCCL_FLOAT32,\n",
        "            nccl.NCCL_SUM,\n",
        "            stream.ptr\n",
        "        )\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        streams[r].synchronize()\n",
        "    print(\"GEMM + ReduceScatter OK\")"
      ],
      "metadata": {
        "id": "sUI-jQ5Zy2YU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def allgather_gemm():\n",
        "    M, K, N = 1024, 512, 1024\n",
        "    n_per_rank = N // WORLD\n",
        "\n",
        "    Arows, Bshard, Ball, C = [], [], [], []\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        A_r = cp.random.rand(M, K, dtype=cp.float32)\n",
        "        B_shard = cp.random.rand(K, n_per_rank, dtype=cp.float32)\n",
        "        B_all = cp.zeros((K, N), dtype=cp.float32)\n",
        "        C_r = cp.empty((M, N), dtype=cp.float32)\n",
        "        Arows.append(A_r); Bshard.append(B_shard)\n",
        "        Ball.append(B_all); C.append(C_r)\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        comm = comms[r]\n",
        "        stream = streams[r]\n",
        "        comm.allGather(\n",
        "            Bshard[r].ravel().data.ptr,                   # send (K*n_per_rank)\n",
        "            Ball[r].ravel().data.ptr,                     # recv (K*N)\n",
        "            K * n_per_rank,                               # sendcount (elements)\n",
        "            nccl.NCCL_FLOAT32,\n",
        "            stream.ptr\n",
        "        )\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        C[r][:] = Arows[r] @ Ball[r]\n",
        "\n",
        "    for r in DEVS:\n",
        "        cp.cuda.Device(r).use()\n",
        "        streams[r].synchronize()\n",
        "    print(\"AllGather + GEMM OK\")"
      ],
      "metadata": {
        "id": "UPjrb-Liy4TO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    all_to_all()\n",
        "    gemm_reducescatter()\n",
        "    allgather_gemm()"
      ],
      "metadata": {
        "id": "kyaX__Bcy9Ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ae6966-2ac8-4813-d62f-fab55a4c5efb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All-to-all OK\n",
            "GEMM + ReduceScatter OK\n",
            "AllGather + GEMM OK\n"
          ]
        }
      ]
    }
  ]
}